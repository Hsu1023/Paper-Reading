HPO有点像regression问题，不过每一个label都需要时间开销的代价，performance和efficiency进行trade-off；好处是可以获得一个模糊的label提高efficiency

1.解耦问题,bz,dim,optimi选adam就够了，超参之间的关系，对domain理解（多阶段似乎可以筛选超参）

2.对图学习的理解，会对小图training，特点，图采样，样本之间可能有关联（有一些子图的代表性更强） Data HP

子图如何采样似乎也是一个超参，Data HP，可以理解为HPO的HP，如何采选子图，缩小数据规模

Model HP缩小搜索空间

BO不会显式缩小空间，我们显式缩小空间，自适应会更好，不借助**先验知识**

搜索空间确定更重要，大图上很难找到搜索空间



acquisition会平衡exploit和

EI

新的acquisition function考虑scale

learning curve-based prediction：直接预测大图上效果结果不是很好，跨scale不是很好



### 一个model贯穿始终（实际上相当于边model边SH）

* 每个stage随机选取一小部分candidates进行评估（我们有一堆弱样本1000 然后训练一些100，选出500个）
* 每个stage选取不同的line作为0,1分界，或者一定比例（似乎可以结合confidence等等或者结合learning curve-based prediction）
* 每次promotion可以打分+随机采用被淘汰的配置

model选取有要求吗，RF/Adaboost/...
多线程
优点：样本量更大，且时间开销小（也许适合大图）



### 多个model：（是相当于先SH再model）

分阶段，前几个阶段epoch和graph可以调很小，获得预训练结果喂给下一stage

解耦问题：可不可以做更多超参解耦（这是KG的特色）

问题：增加stage可能对于最终筛选效果可能改进不大



### 先model再SH：

是BOHB的思路
可以考虑performance improvement/learning curve-based prediction，从BO model training角度结合KG特色优化

BO能学到一些知识，似乎可以起到过滤搜索空间的作用

大的bandit-base效率高，BO效果好

优点：可以筛选出大量可行的SH



### 其他问题

random-forest怎么做的预训练

KG HPO有何特色

transformer